# 统计学习方法

## k-means 聚类

## 朴素贝叶斯
- [ ] **贝叶斯定理** 的含义：条件概率公式。`P(A|B) = P(A)*(P(B|A)/P(B))`。A 的概率（在 B 发生的条件下）是由 A 的先验概率 P(A) 乘以 B 对 A 事件发生的影响因子 `P(B|A)/P(B)` 决定的。
  - 实例：某个女孩的行为 B 推测女孩喜欢自己 A。
- [ ] **朴素贝叶斯法** 的特点：较强的条件独立性假设
- [ ] **贝叶斯估计** 与极大似然估计：极大似然估计（直接把数据统计作为可能性）可能出现概率为 0 的情况，贝叶斯估计在频数上随机附一个正数（lambda=1时，叫拉普拉斯平滑）。

## 决策树

- 决策树的推理：  
每步做决策，关键在训练（构建）树的过程。构建过程希望决策的次数少，即树的深度较小。  
- **纯度** 的度量
  - 熵的计算：  
    - 如何衡量当前情况下的平均信息量（熵）：对单个随机变量推广，到多个随机变量存在的情况。
    - 如何衡量经验条件下的平均信息量（条件熵）：概率相加，平均信息量同样相加，求H(A|B)相当于对所有的i求sum{Pi*H(A|B=bi}  
  - Gini 系数
- **节点划分策略**：  
  - **ID3** 的信息增益：纯度度量于熵减 [link](https://zhuanlan.zhihu.com/p/34534004)，由于**类别数多的特征熵减明显**，ID3 存在一定偏好。[信息增益计算.jpg](https://pic1.zhimg.com/80/v2-f6d10699fdbe216617836c7e8732ba58_720w.jpg)
  - **C4.5** 的信息增益率：ID3 基础上做类别数的惩罚。所以结果 gR(D,A) = g(D,A) / H(D)。为什么这里除以**经验熵** H(D)，H(D) 应当是划分 A 之后的情况。[信息增益率计算.jpg](https://pic2.zhimg.com/80/v2-c35719627c479737cb680c3f4d8cdf6d_720w.jpg)
    - 这会产生对类别数少的特征的偏好，实际先算信息增益，增益高于平均的特征取最大信息增益率的对应特征。
    - example: A/B 两个特征，所有剩余样本对 A 特征完全一对一，这时候信息增益最大，但信息增益率很小。
  - **CART** 的基尼系数，能分类也能回归。[gini.jpg](https://pic3.zhimg.com/80/v2-79214da261d75829046953ab9cb8b03a_720w.jpg)
- **回归树**：穷举所有阈值，纯度的衡量以均方差代替熵的计算。
   **剪枝**
  - 为了泛化，减少过拟合
  - 预剪枝：在划分之前剪枝
    - 数据子集个数小于阈值
    - 所有节点都已分裂
    - 划分后准确率降低
  - 后剪枝 
    - 递归地从下往上判断剪枝前后精度
- **随机森林**：提升方法在决策树的应用。
    - 随机性的体现：数据的随机选取、特征的随机选取（子树划分时，在随机采样的子集做随机选取）。

## LR 逻辑回归
- [ ] 逻辑回归、凸优化、代价函数的推导。[link](https://blog.csdn.net/yan456jie/article/details/52589738)
  - sigmoid 激活函数：适应伯努利分布
  - log 几率：避免非凸函数问题
  - 损失函数：极大似然准则 
  - 求解：梯度下降、坐标下降
- [ ] LR 手推：极大似然和梯度下降 [link1](https://www.cnblogs.com/chen8023miss/p/11308971.html); [link2](https://www.cnblogs.com/bonelee/p/7253508.html)

## SVM 支持向量机

[link1](https://www.cnblogs.com/chen8023miss/p/11308971.html); [link2](
https://zhuanlan.zhihu.com/p/45444502); [link3](
https://blog.csdn.net/qq_39422642/article/details/78725278)
[sigai](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247483937&idx=1&sn=84a5acf12e96727b13fd7d456c414c12&chksm=fdb69fb6cac116a02dc68d948958ee731a4ae2b6c3d81196822b665224d9dab21d0f2fccb329&scene=21#wechat_redirect)

- SVM手推：
  - 最大边界
  - 最大间隔假设
  - 最小化W**2
  - 拉格朗日乘子法优化
  - 转化对偶条件
  - 不等式约束的极值必要性
- 线性核与非线性核；高斯核适合低维、样本数量不多；核方法就是把特征向高维空间映射，就可以用线性核解决高纬度数量较多的问题。

## 提升方法

[zhihu 文章介绍](https://zhuanlan.zhihu.com/p/34534004)

- **组合分类器** 的概念 [组合分类器.jpg](https://pic1.zhimg.com/80/v2-8fc5ff86df06a3c4d3f7d4b055642224_720w.jpg)
  - 好处：平滑决策边界，减少整体错误（所以也不会有太大提升），处理多源异构数据效果显著
  - 抽样方法：数据少时 bagging （有放回），数据多时子集划分（无放回）
  - 优势：处理多源数据、异构数据
- **Bagging** (Boostrap aggregating)：并行拟合。数据量小时，有放回的采样，采样次数与样本总数相同，会有重复
- **Boosting**：串行拟合。增加上一次采样分类错误的样本权重，不断提升。样本权重、集成权重需要我们的考虑。[Boosting.jpg](https://pic4.zhimg.com/80/v2-aca3644ddd56abe1e47c0f45601587c3_720w.jpg)
  - AdaBoost：Boosting 的一个知名策略。
    - 集成权重：当前模型对应的误差率。[分类误差率计算.jpg](https://pic2.zhimg.com/80/v2-3f8463843d3f88642a288666ecb94ff1_720w.jpg)，[集成步骤.jpg](https://pic1.zhimg.com/80/v2-7000a239700933215671f4f66066ddd4_720w.jpg)
    - 样本权重：上次模型的对应样本指标 / 整体指标之和（概率分布），该指标与该样本正确与否、误差率呈指数负相关，与上次的权重呈正相关。[样本权重计算.jpg](https://pic1.zhimg.com/80/v2-8d2590f60815d6389572d4f09ed9a658_720w.jpg)
- **GBDT** (Gradient Boosting Decision Tree)：以 CART 决策树为基的迭代树。[大致流程.jpg](https://pic2.zhimg.com/80/v2-4713a5b63da71ef5afba3fcd3a65299d_720w.jpg)
  - 特点：每次学习前一次结果与真实值的残差（学习误差）
  - 损失的约束 [.jpg](https://pic3.zhimg.com/80/v2-a384924b89b1bdd581cef7d75b56e226_720w.jpg)，包括0-1损失、平方损失、L2损失。
- **XGBoost**：基于GBDT，数值优化、损失函数的功能。
  - **损失** 函数：平方损失、逻辑回归损失、L1/2正则化。借鉴GBDT，约束的对象同样是与之前树判定结果的残差。[损失函数.jpg](https://pic2.zhimg.com/80/v2-1c0706e463f78b6036b3923048ac9149_720w.jpg)
  - **子节点分割** 选择：根据损失函数的推导（略过），得到贪心算法和近似算法的约束条件：[图示.jpg](https://pic4.zhimg.com/80/v2-d0cf0063c23679e711146f861d36fc17_720w.jpg)
    - 贪心算法：最大化：左子树+右子树-分割之前的分数-const复杂度代价
    - [ ] 加权直方图近似算法（本质是抽样算法，可以并行化处理）
  - 相比 GBDT 的 **优势**： 
    - GBDT 只支持 CART 决策树为基，XGBoost 支持线性分类器，相当于带 L1/L2 正则化的逻辑回归、线性回归
    - 优化用到二阶导泰勒展开；
    - 加入正则项，控制模型复杂度；
    - 并行加速：加权直方图近似算法
    - [ ] 等等



## EM 算法与推广

## HMM 隐马尔可夫模型

- 隐马尔可夫模型的基本概念：
  - 有向图模型
    - 理解：P(X1,X2,X3)=P(X1)\*P(X2|X1)\*P(X3|X1,X2)，能通过其次马尔科夫性假设，将P(X3|X1,X2)=P(X3|X2)，简化
  - 1个模型：lambda=(pi, A, B)。pi 为初始概率分布，A 为状态转移概率矩阵，B 为观测概率矩阵。
  - 2个假设
    - 齐次马尔可夫性假设：隐藏的马尔可夫链在任意时刻的状态只依赖于前一时刻的状态
    - 观测独立性假设：任意时刻的观测只依赖于该时刻的马尔可夫链的状态。
  - 3个求解问题：
    - Evaluation：P(O|lambda)
    - learning：lambda
    - decoding：argmax{i}P(I|O)
- 实例
  - 盒子和球的问题。观测到的是每次取出的球，当前选的盒子只与之前的选择有关。
  - 隐马尔可夫链的条件假设很适合语音处理。观测到的是文字，背后的是思考的内容。每次思考后表达的文字与思考的内容强相关。

## CRF 条件随机场

[概率图模型体系](https://zhuanlan.zhihu.com/p/33397147)
- 基本概念
  - 给定一组输入随机变量，给出另一组随机变量的条件概率分布
  - 假设输出随机变量构成马尔可夫随机场。
- 概率无向图模型（马尔可夫随机场）
  - 用无向图表示的联合概率分布。
  - 节点之间的连接关系：条件独立性（马尔可夫性：成对、局部、全局）
- 条件随机场的定义与形式
- 条件随机场的概率计算
- 条件随机场的学习算法
- 条件随机场的预测算法



## 场景问题
- 当分类对象的属性是连续数值时，怎么选取机器学习的方法？  
LR、SVM、神经网络的假设本身就建立在连续的变量空间里；决策树、朴素贝叶斯本身建立在二分类的离散数值基础上。但可以通过取阈值将连续空间离散化来处理连续数值问题。