## 统计学习方法

- [ ] 伯努利分布、二项分布、多项式分布：[link](https://www.zhihu.com/question/50561130/answer/262339774)
- [x] 条件概率分布 [link](https://zhuanlan.zhihu.com/p/26464206)
- [ ] 先验 / 后验概率：经过统计 / 观察某事件后得到的事件概率分布。
- [x] 先验 / 后验概率与极大似然估计：后验概率是知果求因，极大似然是知果求最可能的原因。[link](https://zhuanlan.zhihu.com/p/24423230)


### 朴素贝叶斯
- [ ] 贝叶斯定理的含义：条件概率公式。`P(A|B) = P(A)*(P(B|A)/P(B))`。A 的概率（在 B 发生的条件下）是由 A 的先验概率 P(A) 乘以 B 对 A 事件发生的影响因子 `P(B|A)/P(B)` 决定的。
  - 实例：某个女孩的行为 B 推测女孩喜欢自己 A。
- [ ] 朴素贝叶斯法的特点：较强的条件独立性假设
- [ ] 贝叶斯估计与极大似然估计：极大似然估计（直接把数据统计作为可能性）可能出现概率为0的情况，贝叶斯估计在频数上随机附一个正数（lambda=1时，叫拉普拉斯平滑）。

### 决策树

- 决策树的推理：  
每步做决策，关键再训练（构建）树的过程。构建过程希望决策的次数少，即树的深度较小。  
- 熵的计算：  
如何衡量当前情况下的平均信息量（熵）：对单个随机变量推广，到多个随机变量存在的情况。  
如何衡量经验条件下的平均信息量（条件熵）：概率相加，平均信息量同样相加，求H(A|B)相当于对所有的i求sum{Pi*H(A|B=bi}  
- 信息增益：  
ID3的信息增益  
C4.5的信息增益率：如果划分的方法，给划分的结果带来很大不确定性，这种划分是不合理的。所以结果gR(D,A) = g(D,A)/H(D)。为什么这里除以经验熵H(D)，H(D)应当是划分A之后的情况。
  - example: A/B两个特征，所有剩余样本对 A 特征完全一对一，这时候信息增益最大，但信息增益率很小。

### LR 逻辑回归
- [ ] 逻辑回归、凸优化、代价函数的推导。[link](https://blog.csdn.net/yan456jie/article/details/52589738)
- [ ] LR 手推：极大似然和梯度下降 [link1](https://www.cnblogs.com/chen8023miss/p/11308971.html); [link2](https://www.cnblogs.com/bonelee/p/7253508.html)

### SVM 支持向量机
- SVM手推：最大边界、最大间隔假设、最小化W**2、拉格朗日乘子法优化、转化对偶条件、不等式约束的极值必要性、线性核与非线性核；高斯核适合低维、样本数量不多；核方法就是把特征向高维空间映射，就可以用线性核解决高纬度数量较多的问题。[link1](https://www.cnblogs.com/chen8023miss/p/11308971.html); [link2](
https://zhuanlan.zhihu.com/p/45444502); [link3](
https://blog.csdn.net/qq_39422642/article/details/78725278)

### 提升方法

### EM 算法与推广

### HMM 隐马尔可夫模型

### CRF 条件随机场


### 场景问题
- 当分类对象的属性是连续数值时，怎么选取机器学习的方法？  
LR、SVM、神经网络的假设本身就建立在连续的变量空间里；决策树、朴素贝叶斯本身建立在二分类的离散数值基础上。但可以通过取阈值将连续空间离散化来处理连续数值问题。