
## 基础
- 正则化：L1 和 L2 正则化的区别
  - 服从分布：L1 拉普拉斯分布，L2 高斯分布
  - 参数稀疏性：L1 正则化更稀疏
  - 参数方差：L2 训练结果方差较小
- L2 **正则化** 参数衰减的原因：计算梯度叠加到原有参数，乘以系数 `1-lr*lambda`。[link](https://blog.csdn.net/obrightlamp/article/details/85290929)

## 优化问题

- 常见优化策略 [link](https://www.cnblogs.com/guoyaohua/p/8542554.html)
- BGD 训练集整体
- SGD 单样本
- MBGD batch 梯度下降
- batch 大小选择：如果 m<=2000，直接用 batch 梯度下降，否则取典型值2^n
- Momentum 动量梯度算法：**指数加权平均** 与动量梯度算法 [link](https://zhuanlan.zhihu.com/p/32335746)
- RMSprop 幅度约束梯度算法：梯度下除以计算系数 S，这个系数也采用动量思想，与上次计算和当前梯度数值有关
- Adam：Momentum 方向梯度 + RMSprop 幅值梯度
- 学习率衰减方法：手工设定、指数衰减（以0.95为底，epoch为指数）、指标不再变化即调整
- 梯度消失与梯度爆炸
  - 初始化缓解方法：[link](https://zhuanlan.zhihu.com/p/64464584)
  - BN 批归一化：稳定分布
- 为什么梯度方向函数值下降最快？把高维梯度看做一维梯度大小方向的叠加，一维梯度的含义是当前点的变化率。
- 局部 **最优** 问题：
  - 为什么神经网络损失大多是凸函数
  - 凸函数：二阶导大于 0
  - [ ] 凸优化 https://zhuanlan.zhihu.com/p/51127402


## 层的实现
- [Caffe 层的实现](../3-计算机视觉/3.1%20基础/3.1.4%20Caffe%20源码.md)
- [x] Dropout 的实现：反向随机失活 inverted dropout 保证期望不变、代价函数不被明确定义缺点） [link](https://blog.csdn.net/bestrivern/article/details/85273238)
- [ ] CNN
- [ ] BatchNorm，与 GN 的关系 [link](https://blog.csdn.net/qq_41997920/article/details/89945972)


## 时序专题
- RNN
  - simple RNN
  - LSTM (Long Short-Term Memory)
    - 乘法忘记门
    - 加法记忆门
    - 乘法输出门
  - GRU
- Attention CNN
- [ ] bert transform 等，CNN处理非LSTM处理时序的方法-NLP：主要缺点训练时间比较长


## 可视化专题


- [ ] Gram-CAM 的实现