
## 神经网络

### 优化问题
- L2 **正则化**为参数衰减的原因：计算梯度叠加到原有参数，乘以系数`1-lr*lambda`。[link](https://blog.csdn.net/obrightlamp/article/details/85290929)
- 指数加权平均与动量 **梯度** 算法 [link](https://zhuanlan.zhihu.com/p/32335746)
- Adam=动量梯度算法+幅度约束梯度算法RMSprop（梯度下除以计算系数S，这个系数也采用动量思想，与上次计算和当前梯度数值有关）
- 学习率衰减方法：手工设定、指数衰减（以0.95为底，epoch为指数）、指标不再变化即调整
- 梯度消失与梯度爆炸：利用初始化缓解
- 参数初始化方法：[link](https://zhuanlan.zhihu.com/p/64464584)
- 为什么梯度方向函数值下降最快？把高维梯度看做一维梯度大小方向的叠加，一维梯度的含义是当前点的变化率。
- batch梯度下降、随机梯度下降、mini-batch梯度下降
- batch大小选择：如果m<=2000，直接用btach梯度下降，否则取典型值2^n
- 局部 **最优** 问题：
- 为什么神经网络损失大多是凸函数
- 凸函数与凸优化：凸函数：二阶导大于0 TODO https://zhuanlan.zhihu.com/p/51127402


### 层的实现
- [x] Dropout的实现：反向随机失活inverted dropout保证期望不变、代价函数不被明确定义缺点） [link](https://blog.csdn.net/bestrivern/article/details/85273238)
- [ ] CNN
- [ ] BatchNorm


### 时序专题
- RNN
  - simple RNN
  - LSTM (Long Short-Term Memory)
  - GRU
- Attention CNN
- [ ] bert transform 等，CNN处理非LSTM处理时序的方法-NLP：主要缺点训练时间比较长